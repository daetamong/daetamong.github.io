---
layout: post
title: Gradient Descent(경사하강법)이란?
subtitle: Gradient Descent의 개념에 대해 알아보자
categories: 머신러닝
tags: [머신러닝]
---


`개인적으로 공부한 내용을 포스팅하기 때문에 잘못된 내용이 있을 수 있습니다. 만약 틀린 내용이 있다면 적극적인 피드백 부탁드립니다^^`

### Gradient Descent(경사하강법)이란?

우선 경사하강법이 뭔지 바로 정리하고 넘어가려고 한다. 왜냐하면 대략적으로 어떤 개념인지 알고 시작하는게 이후에 있을 내용을 이해하는 데 도움이 되지 않을까라는 개인적인 생각 때문이다.

그래서 경사하강법이 먼지 정리를 해보자면..

> **경사하강법은** Cost function을 최소화하는 알고리즘으로,  가중치를 반복적으로 증가/감소시켜가며 함수 값이 최소가 되도록(=기울기가 0이 되도록) 하는 알고리즘이다.

이렇게만 정리를 하면 나도 이해가 잘 되지 않고, 우연히(?) 이 글을 보는 사람들도 이해가 가지 않을 수 있다.

그래서 왜 머신러닝, 딥러닝에서 경사하강법이 중요한 개념 중 하나로 인식되고 있는지, 그리고 이 개념을 실제 모델 학습에서 어떻게 쓰는지 차례차례 살펴보면서 이해해보려 한다.



### Cost Function(=비용함수)이란?

여기서 또 새로운 개념이 하나가 더 있는데 '비용함수'라는 개념이 있다.

비용함수는 **"최적의 가중치를 찾기 위한 기준이 되는 함수"**이다.

머신러닝에서는 비용함수가 최소가 되도록 최적의 파라미터를 찾는 과정을 최적화라고 하는데, 이 때 사용하는 기준이 비용함수이다.

비용함수에도 여러가지 식이 있는데, 대표적인으로 MSE(Mean Squared Error, 평균 제곱 오차)와 MAE(Mean Absolute Error, 평균 절대 오차)가 있다.

하나씩 살펴보자!

> 1. MSE = $\frac1n\sum_{i=1}^n(y_i - \hat{y})^2$

> 2. MAE = $\frac1n\sum_{i=1}^n\vert y_i - \hat{y}\vert$


위 두 식의 차이는 제곱을 했는가 아니면 절대값을 씌었는가라는 차이점이 있는데, 물론 상황에 따라 다르겠지만 일반적으로는 제곱을 한 MSE를 주로 사용한다.

그 이유는 크게 두 가지 정도가 있다.

첫 번째 이유로는 실제값($y_i$)과 예측값($\hat{y}^2$)의 차이를 더 크게 만들어줌으로써 업데이트할 가중치에 더 큰 패널티를 부여하기 위함이다.

두 번째는 컴퓨터 연산량의 이류로 절대값을 씌운 case에 좀 더 많은 연산량이 부여된다고 한다. 따라서 연산량 부하를 조금이라도 줄이기 위해 절대값보다는 제곱을 씌운 MSE를 더 많이 사용한다.



### 경사하강법을 사용하는 방법은 무엇일까?

이제 경사하강법을 언제 쓰는지를 간단한 예시를 통해 한번 알아보자!

우리는 [1,2,3]이라는 Input Data를 갖고 있고, 이 데이터를 어떤 회귀 모형에 넣었을 때 [3,5,7]이라는 결과를 얻었을 때, 우리는 회귀모형식을 알고 싶다고 하자.

데이터가 3개 밖에 없으니 우리가 직접 손으로 계산하여 회귀모형이 대략 f(x) = 2x + 1가 되지 않을까 유추할 수 있다.

그런데, 만약 수십만개의 데이터가 있다면 과연 우리가 모형식을 손으로 계산할 수 있을까?

이처럼 직접 계산하기가 현실적으로 불가능한 경우에 모형식의 파라미터를 추정할 때 경사하강법을 사용한다.

위의 데이터를 그려보면 아래처럼 점이 찍힐 것이다. 그리고 주황색 선처럼 모형식을 유추하여 그릴 수 있을 것이다.

![do-messenger_screenshot_2023-11-16_15_26_00](https://github.com/daetamong/daetamong.github.io/assets/111731468/269f2789-e812-410e-9881-0811e0f3142b)

하지만 만약 첫 추정 모형식을 y = x라고 하자. 그렇다면 아래의 그림처럼 실제 데이터와 모형식 간의 오차가 발생할 것이다.

![do-messenger_screenshot_2023-11-16_15_30_40](https://github.com/daetamong/daetamong.github.io/assets/111731468/4c66e97d-5c25-4c96-930f-452bf96436c7)

그럼 이제 우리는 이 오차가 적어지도록 파라미터를 다시 재지정해줘야 한다.

이 때, 실제값과 예측값의 차이를 계산하여 얼마나 차이가 있는지 확인을 하는데, 이 때 사용하는 식이 위에서 정리한 cost function이다.

그럼 우리는 이 cost function을 최소화하기 위해 다시 한번 모형식의 파라미터(기울기, 절편)을 구한다.

이 때, 구하는 방법이 경사하강법이다.

우선 첫 번째의 cost function을 MSE로 계산해보자!

> MSE = $\frac1n(wx_i + b - y_i)^2 = \frac13(2^2 + 3^2 + 4^2) = 29/3$

앞으로 우리는 MSE를 cost function으로 사용할 것이다.



### Cost Function을 최소화해보자!

우선 Cost function은 W(=기울기)와 b(절편)에 대해서 2차 함수 형태를 띄고 있다.

따라서 W와 b 그리고 cost function의 관계를 그래프로 나타내보면 2차 함수 형태로 표현할 수 있다.

![do-messenger_screenshot_2023-11-16_16_46_39](https://github.com/daetamong/daetamong.github.io/assets/111731468/8d99fced-a002-461f-9baa-41b5e691308b)


그래서 우리는 이 2차 함수에서 cost가 최소가 되는 지점을 찾으면 되는데 이를 **"Global optimum"**이라고 한다.

이 Global optimum은 가장 작은 cost를 의미하는데, 이는 곧 이 그래프에서의 기울기가 0이 되는 부분과 동일하다.

따라서 우리는 최소의 cost를 찾는 것은 이 함수의 기울기(=Gradient)가 0이 되는 부분을 찾는 것과 같은 개념이라고 보면 된다.

위의 그래프를 보면 우리가 처음으로 정의한 모형식은 y=x였다. 그래서 w는 1이 되고, 이에 따른 cost에 대한 기울기는 왼쪽 점선을 의미한다.

하지만 우리는 기울기가 0인 지점을 아직 찾지 못했기 때문에 여기서 끝나는게 아니라 다음 step을 밟는다. (b도 동일한 과정을 거친다)

그 과정은 다음과 같다.

![do-messenger_screenshot_2023-11-16_16_56_10](https://github.com/daetamong/daetamong.github.io/assets/111731468/6c59c474-a0ff-4615-b00c-bc1f6e1457f2)

> 기울기가 음수라면, 오른쪽으로 이동

> 기울기가 양수라면, 왼쪽으로 이동

위와 같은 과정을 반복하면서 즉, cost를 계산해나가면서 기울기가 0인 지점을 점차적으로 찾아나가야 한다.

대충 그래프로는 이해를 했으니, 실제 식을 계산해보며 수학적으로도 이해를 해보자!

우리는 2차 함수 형태를 띄는 그래프의 기울기를 구해야 한다. ASDSP 기울기를 구하기 위해서는 미분을 해야 하는데, 우리는 w와 b에 대해서 각각 미분을 해야 하기 때문에 각각에 대하여 편미분을 해야 한다. (귀찮으니 w만 구해보자. 구하는 과정은 동일하다)


> $\sum_{i=1}^n(Wx_{i} + b - y_{i})^{2}$ = $ \sum_{i=1}^n(x_{i}^{2}W_i + 2x_{i}bW -2by_{i} - 2x_{i}y_{i}W + b^{2} + y_{i}^{2})$


우선, 그래프의 식을 풀었다.

그 다음 W와 b에 대하여 각각 편미분을 한다.

여기서는 W에 대해서만 편미분을 해보겠다. (b를 구하는 식도 동일하다)


> $\frac{\partial {Cost(W,b)}}{\partial W}$ = $\frac1n\sum_{i=1}^{n}(2Wx_{i}^2 + 2x_{i}b - 2x_{i}y_{i})$

위처럼 계산함으로써 W와 b의 기울기를 각각 구할 수 있다.

계산한 결과를 바탕으로 우리는 다음 step의 기울기를 구할 수 있다.

수식을 다시 써보면..

> $W := W -/+ (\eta) \cdot \frac{\partial Cost(W,b)}$

이전 step의 W에서 기울기가 음수이면 더해주고, 양수이면 빼주면서 조금씩 globla optimum으로 수렴하는 방향으로 계산을 이어나가면 된다.



### Learning Rate (학습률)

이제 여기서 또 중요한 개념을 하나 짚고 넘어가야 한다. 위의 식을 보면 $\eta$가 있는데 이 것을 **learning rate**라고 한다.

learning rate는 **학습률**이라고 불린다. learning rate를 사용하는 이유는 위의 식처럼 다음의 기울기를 빼주거나 더해주는데 이 기울기의 값이 너무 커버리면 수렴하지 않고 오히려 발산하는 문제가 발생할 수 있다. 따라서 이러한 문제를 해결하기 위해 기울기에 적은 값을 곱해줌으로써 발산하는 것을 방지할 수 있다. 이 때 사용하는 하나의 파라미터를 learning rate 즉, "학습률"이라고 한다.

### 왜 발산할까?

발산하는 문제를 좀 더 자세히 살펴보자!

우리는 매 최적화 단계마다 기울기를 구하게 되는데, $W := W -/+ (\eta) \cdot \frac{\partial Cost(W,b)}$ 의 식처럼 다음 step으로 넘어가기 위한 이동거리를 빼주거나 더해줘야 한다.

이 때의 이동거리는 이전 단계에서 구한 기울기로 사용한다. 그 이유는 기울기가 극소값(=Global optimum)게 가까워질수록 그 값은 작아지기 때문이다.

다시 쉽게 정리를 하자면,, 현재의 x값이 극소값에서 멀면 기울기가 클 수 밖에 없는데, 이러면 극소값으로 가기 위해서는 이동거리가 커야 한다.

반대로 x값이 극소값에 가까이 있다면 조금만 움직여도 극소값에 가까워질 수 있다.

그래서 기울기에 따라 다음 step에서 얼만큼 이동해야 하는지를 대략적으로 알 수 있기 때문에 이동거리를 기울기, 즉 Gradient로 사용하는 것이다.

따라서, 현재 상태의 위치에서 **learning rate와 Gradient**를 곱한 값을 빼주거나 더해줌으로써 극소값에 수렴하도록 하는 것이다.



### Learning rate의 크기에 따라 어떻게 바뀔까?

우리는 한 가지 더 고려해야 하는 것이 있다. Learning rate를 지정해줘야 하는데, 지정해주는 값에 따라 연산량의 차이도 있을 뿐더러 때로는 모델의 성능에도 영향을 미칠 수가 있다. 따라서 우리는 학습률을 적절한 값으로 지정해줘야 하는 경우도 고려해야 한다.

대략 0.001, 0.01 등과 같은 적은 수를 지정해준다.

![do-messenger_screenshot_2023-11-16_17_23_26](https://github.com/daetamong/daetamong.github.io/assets/111731468/4f0dbcf9-2640-4843-8ecd-7541453a6978)

만약 learning rate를 큰 값으로 지정해준다면, 왼쪽의 그래프처럼 이동거리가 커지면서 global optimum으로 수렴하지 않고 발산하는 경우가 생길 수가 있다. 

반대로 learning rate를 엄청 작은 값으로 해준다면, 극소값으로 갈 수는 있겠지만, 수많은 과정을 반복함으로써 연산량도 많아질뿐더러 "local minima"에 빠지는 문제가 생길 수 있다.

따라서 우리는 힘들겠지만? 적절한 learning rate를 찾는 것도 중요하게 생각해둬야 할 필요가 있다.


### 정리를 하자면..?
> 1. 경사하강법은 모형식의 적절한 파리미터를 찾아주는 알고리즘이다.
> 2. 실제값과 예측값의 차이를 계산한 Cost function을 통해서 모형식의 파라미터(기울기, 절편)를 추정한다.
> 3. 함수의 기울기를 구하여 기울기가 낮은 쪽으로 즉, 극소점으로 이동하게끔 기울기를 계속해서 변화시켜 나간다.
> 4. 기울기가 0이 되는 지점, 극소점을 찾는 순간의 x축값이 곧 최적의 파라미터가 된다.
> 5. 모든 파라미터에 대하여 위 방식을 동일하게 적용하여 최적의(=Cost function을 최소화하는) 모형식을 추정한다.

<br>
<hr>
`다음 포스팅에서는 Gradient Descent를 직접 Python으로 구현해보는 실습을 해보도록 하겠다!!!`